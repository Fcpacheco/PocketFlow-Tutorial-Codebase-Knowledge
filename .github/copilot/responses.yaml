# Agentic Coding: Humans Design, Agents code!
behavior: |
  If you are an AI agent involved in building LLM Systems, read this guide VERY, VERY carefully! This is the most important chapter in the entire document. Throughout development, you should always:
  
  1. Start with a small and simple solution
  2. Design at a high level (docs/design.md) before implementation
  3. Frequently ask humans for feedback and clarification
  
  Agentic Coding is a collaborative process between Human System Design and Agent Implementation, with humans understanding the requirements and AI assisting with implementation.

# Agentic Coding Steps
agentic_coding_steps: |
  Agentic Coding should be a collaboration between Human System Design and Agent Implementation:
  
  | Steps                  | Human      | AI        | Comment                                                               |
  |:-----------------------|:----------:|:---------:|:----------------------------------------------------------------------|
  | 1. Requirements        | ★★★ High   | ★☆☆ Low   | Humans understand the requirements and context.                       |
  | 2. Flow                | ★★☆ Medium | ★★☆ Medium| Humans specify the high-level design, AI fills in details.            |
  | 3. Utilities           | ★★☆ Medium | ★★☆ Medium| Humans provide APIs and integrations, AI helps with implementation.    |
  | 4. Node                | ★☆☆ Low    | ★★★ High  | AI designs node types and data handling based on the flow.             |
  | 5. Implementation      | ★☆☆ Low    | ★★★ High  | AI implements the flow based on the design.                            |
  | 6. Optimization        | ★★☆ Medium | ★★☆ Medium| Humans evaluate results, AI helps optimize.                            |
  | 7. Reliability         | ★☆☆ Low    | ★★★ High  | AI writes test cases and addresses corner cases.                       |
  
  1. **Requirements**: Clarify requirements and evaluate if an AI system is a good fit. 
      - Understand AI systems' strengths and limitations:
        - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
        - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
        - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
      - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
      - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

  2. **Flow Design**: Outline how your AI system orchestrates nodes at a high level.
      - Identify applicable design patterns (e.g., Map Reduce, Agent, RAG).
        - For each node in the flow, start with a high-level one-line description of what it does.
        - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
        - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
        - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
      - Outline the flow, preferably with diagrams for visualization.

  3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.
      - Think of your AI system as the brain. It needs a body—these *external utility functions*—to interact with the real world:
          - Reading inputs (e.g., retrieving messages, reading emails)
          - Writing outputs (e.g., generating reports, sending emails)
          - Using external tools (e.g., calling LLMs, searching the web)
          - **NOTE**: *LLM-based tasks* (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; they are *core functions* internal to the AI system.
      - For each utility function, implement it and write a simple test.
      - Document their input/output, as well as why they are necessary.

  4. **Node Design**: Plan how each node will read and write data, and use utility functions.
      - Use a shared store approach (in-memory dictionary or database).
      - For simple systems, use an in-memory dictionary.
      - For more complex systems or when persistence is required, use a database.
      - For each Node, describe its type, how it reads and writes data, and which utility function it uses.
      - Keep it specific but high-level without code.

  5. **Implementation**: Implement initial nodes and flows based on the design.
      - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
      - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
      - Add logging throughout the code to facilitate debugging.

  6. **Optimization**:
      - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
      - **Redesign Flow**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
      - For micro-optimizations:
        - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
        - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

  7. **Reliability**:
      - **Node Retries**: Add checks in node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
      - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
      - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

# Best Practices
best_practices: |
  - **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.
  - **Sometimes, design Utilities before Flow:** For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
  - **Keep it user-centric:** Explain problems from the user's perspective rather than just listing features.
  - **Balance complexity vs. impact:** Deliver high-value features with minimal complexity early.
  - **You'll likely iterate a lot!** Expect to repeat Steps 3–6 hundreds of times.

# Project Structure Guidelines
project_structure: |
  ## Example LLM Project File Structure

  ```
  my_project/
  ├── main.py               # Entry point
  ├── nodes.py              # Node definitions
  ├── flow.py               # Flow orchestration
  ├── utils/                # Utility functions
  │   ├── __init__.py
  │   ├── call_llm.py       # One file per API call
  │   └── search_web.py
  ├── requirements.txt
  └── docs/
      └── design.md         # High-level documentation
  ```

  - **`docs/design.md`**: Contains project documentation for each step. This should be *high-level* and *no-code*.
  - **`utils/`**: Contains all utility functions.
    - It's recommended to dedicate one Python file to each API call, for example `call_llm.py` or `search_web.py`.
    - Each file should also include a `main()` function to try that API call.
  - **`nodes.py`**: Contains all the node definitions.
  - **`flow.py`**: Implements functions that create flows by importing node definitions and connecting them.
  - **`main.py`**: Serves as the project's entry point.

  **Example Node Implementation:**
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # Get question directly from user input
          user_question = input("Enter your question: ")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # Store the user's question
          shared["question"] = exec_res
          return "default"  # Go to the next node

  class AnswerNode(Node):
      def prep(self, shared):
          # Read question from shared
          return shared["question"]
      
      def exec(self, question):
          # Call LLM to get the answer
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # Store the answer in shared
          shared["answer"] = exec_res
  ```

  **Example Flow Implementation:**
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """Create and return a question-answering flow."""
      # Create nodes
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # Connect nodes in sequence
      get_question_node >> answer_node
      
      # Create flow starting with input node
      return Flow(start=get_question_node)
  ```

  **Example Main Function:**
  ```python
  # main.py
  from flow import create_qa_flow

  def main():
      shared = {
          "question": None,  # Will be populated by GetQuestionNode
          "answer": None     # Will be populated by AnswerNode
      }

      # Create the flow and run it
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"Question: {shared['question']}")
      print(f"Answer: {shared['answer']}")

  if __name__ == "__main__":
      main()
  ```

# Node Implementation
node_implementation: |
  A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

  1. `prep(shared)`
     - **Read and preprocess data** from `shared` store. 
     - Examples: *query DB, read files, or serialize data into a string*.
     - Return `prep_res`, which is used by `exec()` and `post()`.

  2. `exec(prep_res)`
     - **Execute compute logic**, with optional retries and error handling.
     - Examples: *(mostly) LLM calls, remote APIs, tool use*.
     - ⚠️ This should be only for compute and **NOT** access `shared`.
     - ⚠️ If retries enabled, ensure idempotent implementation.
     - Return `exec_res`, which is passed to `post()`.

  3. `post(shared, prep_res, exec_res)`
     - **Postprocess and write data** back to `shared`.
     - Examples: *update DB, change states, log results*.
     - **Decide the next action** by returning a *string* (`action = "default"` if *None*).

  **Fault Tolerance & Retries:**
  You can **retry** `exec()` if it raises an exception via two parameters:
  - `max_retries` (int): Max times to run `exec()`. Default is `1` (**no** retry).
  - `wait` (int): Time to wait (in **seconds**) before next retry. Default is `wait=0`.

  **Graceful Fallback:**
  To gracefully handle exceptions after all retries, override:
  ```python 
  def exec_fallback(self, prep_res, exc):
      # By default, it raises the exception
      # But you can return a fallback result instead
      return "Fallback result"
  ```

  The current retry count (0-based) is accessible via `self.cur_retry`.

# Flow Implementation
flow_implementation: |
  A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

  **Action-based Transitions:**
  Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

  Define transitions with the syntax:

  1. **Basic default transition**: `node_a >> node_b`
     - This means if `node_a.post()` returns `"default"`, go to `node_b`. 
     - (Equivalent to `node_a - "default" >> node_b`)

  2. **Named action transition**: `node_a - "action_name" >> node_b`
     - This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

  **Creating a Flow:**
  - A Flow begins with a **start** node: `flow = Flow(start=some_node)`
  - When you call `flow.run(shared)`, it executes the start node, looks at the Action from `post()`, follows the transition, and continues.

  **Nested Flows:**
  - A Flow can act like a Node, enabling powerful composition patterns.
  - You can use a Flow as a Node within another Flow's transitions.
  - You can combine multiple smaller Flows into a larger Flow for reuse.
  - Node `params` will be a merging of **all** parents' `params`.

  **Important notes:**
  - `node.run(shared)` runs that node alone, but doesn't proceed to the successor.
  - Always use `flow.run(shared)` in production to ensure the full pipeline runs correctly.

# Batch and Async
advanced_patterns: |
  **Batch Processing:**
  Batch makes it easier to handle large inputs in one Node or rerun a Flow multiple times:
  
  1. **BatchNode**:
     - `prep(shared)`: returns an **iterable** (e.g., list, generator).
     - `exec(item)`: called **once** per item in that iterable.
     - `post(shared, prep_res, exec_res_list)`: receives a **list** of results.
     
  2. **BatchFlow**:
     - Runs a **Flow** multiple times, each time with different `params`.
     - Think of it as a loop that replays the Flow for each parameter set.
     
  3. **Nested Batches**:
     - You can nest BatchFlows for hierarchical data processing.
     - Each level merges its params with the parent's.
  
  **Async Processing:**
  For I/O-bound tasks:
  
  1. **AsyncNode**:
     - Implements `prep_async()`, `exec_async()`, `exec_fallback_async()`, and/or `post_async()`.
     - For fetching/reading data asynchronously, async LLM calls, awaiting user feedback, etc.
     - Must be wrapped in an `AsyncFlow`.
  
  2. **AsyncParallel**:
     - Parallel Nodes and Flows run multiple **Async** Nodes and Flows **concurrently**.
     - Use `AsyncParallelBatchNode` or `AsyncParallelBatchFlow`.
     - Best for I/O-bound tasks, not CPU-bound tasks (due to Python's GIL).
     - Be aware of rate limits when making parallel API calls.
  
  **Communication Patterns:**
  Nodes and Flows communicate in 2 ways:
  
  1. **Shared Store** (for almost all cases):
     - A global data structure (often an in-memory dict) that all nodes can access.
     - Use for data results, large content, or anything multiple nodes need.
  
  2. **Params** (primarily for Batch):
     - Local, ephemeral dict passed by the parent Flow, used as an identifier for tasks.
     - Good for immutable identifiers like filenames or IDs.
